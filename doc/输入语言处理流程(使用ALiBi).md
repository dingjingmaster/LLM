# 输入语言处理流程(使用ALiBi)

1. 分词(Tokenization)
   - 把自然语言文本(英文、中文、代码等)分成token
   - 每个token又一个索引id
2. 嵌入(Embedding Layer)
   - 用一个词表嵌入矩阵E把token id映射成向量: $x_{i} = E_{token_{i}}$
3. 送入Transformer块(一般会加位置编码(PE), 但如果用ALiBi, 就不需要在输入层显式加位置向量。换句话说, 输入语言的token向量就是纯词向量，没有RoPE或PE的修饰)
4. 注意力机制 + ALiBi偏置
5. Softmax & Value
   - 经过softmax得到注意力权重, 再和Value结合输出


ALiBi对输入语言的意义:
- RoPE/Sinusoidal PE: 对输入 token embedding加位置编码 -> 每个 token 的表示 = 词义 + 位置
- ALiBi: 输入token embedding = 纯词义, 不包含显式位置。位置信息只在注意力分数中体现

优势:
- 输入层更简单(不加位置编码)
- 对于任意语言(中文, 英文, 代码)处理方式完全一样
- 长文本泛化更好: 因为ALiBi偏置距离线性增长, 模型在短训练时候学到的"远距离衰减"规律自然迁移到更长序列。

例子:
```c
// 1. 输入句子： "我爱AI"
// 2. 分词后: ["我", "爱", "AI"]
// 3. 嵌入后(假设维度=4)
//    x0 = [0.2, 0.1, 0.3, 0.7]
//    x1 = [0.5, 0.9, 0.1, 0.2]
/     x2 = [0.8, 0.1, 0.6, 0.4]
// 4. RoPE: 会在这三个向量里注入位置信息(旋转)
// 5. ALiBi: 保持这三个embedding不变, 只在计算注意力分数时候对 i-j 加上偏置
```